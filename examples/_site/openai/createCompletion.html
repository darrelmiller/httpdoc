<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8">
      <title>Creates a completion for the provided prompt and parameters. | httpdocs </title>
      <meta name="viewport" content="width=device-width, initial-scale=1.0">
      <meta name="title" content="Creates a completion for the provided prompt and parameters. | httpdocs ">
      
      <link rel="icon" href="../favicon.ico">
      <link rel="stylesheet" href="../public/docfx.min.css">
      <link rel="stylesheet" href="../public/main.css">
      <meta name="docfx:navrel" content="../toc.html">
      <meta name="docfx:tocrel" content="toc.html">
      
      
      
      
      <meta name="docfx:docurl" content="https://github.com/darrelmiller/httpdoc/blob/main/examples/openai/createCompletion.md/#L1">
      <meta name="loc:inThisArticle" content="In this article">
      <meta name="loc:searchResultsCount" content="{count} results for &quot;{query}&quot;">
      <meta name="loc:searchNoResults" content="No results for &quot;{query}&quot;">
      <meta name="loc:tocFilter" content="Filter by title">
      <meta name="loc:nextArticle" content="Next">
      <meta name="loc:prevArticle" content="Previous">
      <meta name="loc:themeLight" content="Light">
      <meta name="loc:themeDark" content="Dark">
      <meta name="loc:themeAuto" content="Auto">
      <meta name="loc:changeTheme" content="Change theme">
      <meta name="loc:copy" content="Copy">
      <meta name="loc:downloadPdf" content="Download PDF">
  </head>

  <script type="module" src="./../public/docfx.min.js"></script>

  <script>
    const theme = localStorage.getItem('theme') || 'auto'
    document.documentElement.setAttribute('data-bs-theme', theme === 'auto' ? (window.matchMedia('(prefers-color-scheme: dark)').matches ? 'dark' : 'light') : theme)
  </script>


  <body class="tex2jax_ignore" data-layout="" data-yaml-mime="">
    <header class="bg-body border-bottom">
      <nav id="autocollapse" class="navbar navbar-expand-md" role="navigation">
        <div class="container-xxl flex-nowrap">
          <a class="navbar-brand" href="../index.html">
            <img id="logo" class="svg" src="../logo.svg" alt="httpdocs">
            httpdocs
          </a>
          <button class="btn btn-lg d-md-none border-0" type="button" data-bs-toggle="collapse" data-bs-target="#navpanel" aria-controls="navpanel" aria-expanded="false" aria-label="Toggle navigation">
            <i class="bi bi-three-dots"></i>
          </button>
          <div class="collapse navbar-collapse" id="navpanel">
            <div id="navbar">
            </div>
          </div>
        </div>
      </nav>
    </header>

    <main class="container-xxl">
      <div class="toc-offcanvas">
        <div class="offcanvas-md offcanvas-start" tabindex="-1" id="tocOffcanvas" aria-labelledby="tocOffcanvasLabel">
          <div class="offcanvas-header">
            <h5 class="offcanvas-title" id="tocOffcanvasLabel">Table of Contents</h5>
            <button type="button" class="btn-close" data-bs-dismiss="offcanvas" data-bs-target="#tocOffcanvas" aria-label="Close"></button>
          </div>
          <div class="offcanvas-body">
            <nav class="toc" id="toc"></nav>
          </div>
        </div>
      </div>

      <div class="content">
        <div class="actionbar">
          <button class="btn btn-lg border-0 d-md-none" style="margin-top: -.65em; margin-left: -.8em" type="button" data-bs-toggle="offcanvas" data-bs-target="#tocOffcanvas" aria-controls="tocOffcanvas" aria-expanded="false" aria-label="Show table of contents">
            <i class="bi bi-list"></i>
          </button>

          <nav id="breadcrumb"></nav>
        </div>

        <article data-uid="">

<!DOCTYPE html><html><head><title>Creates a completion for the provided prompt and parameters.</title><link rel="stylesheet" href="../OpenApi.css"><meta charset="utf-8"><meta name="viewport" content="width=device-width, initial-scale=1"></head><body><article><section class="requestOverview"><h1 class="requestSummary">Creates a completion for the provided prompt and parameters.</h1><p class="requestDescription"></p></section><section class="http"><h3>HTTP Request</h3><pre class="httpExample"><span class="requestLine">POST</span> <span class="httpTarget">\completions</span> <span class="httpVersion">HTTP/1.1</span>
<p><span class="headerLine">host</span>: <span class="headerValue">api.openai.com:443</span>
<span class="headerLine">accept</span>: <span class="headerValue">application/json</span>
<span class="headerLine">content-type</span>: <span class="headerValue">application/json</span>
</p></pre></section><dl class="parameters"><h3>Parameters</h3></dl><section class="requestContent"><h3>Request Body Schema</h3><pre class="schema">{
&quot;required&quot;: [
&quot;model&quot;,
&quot;prompt&quot;
],
&quot;type&quot;: &quot;object&quot;,
&quot;properties&quot;: {
&quot;model&quot;: {
&quot;anyOf&quot;: [
{
&quot;type&quot;: &quot;string&quot;
},
{
&quot;enum&quot;: [
&quot;babbage-002&quot;,
&quot;davinci-002&quot;,
&quot;gpt-3.5-turbo-instruct&quot;,
&quot;text-davinci-003&quot;,
&quot;text-davinci-002&quot;,
&quot;text-davinci-001&quot;,
&quot;code-davinci-002&quot;,
&quot;text-curie-001&quot;,
&quot;text-babbage-001&quot;,
&quot;text-ada-001&quot;
],
&quot;type&quot;: &quot;string&quot;
}
],
&quot;description&quot;: &quot;ID of the model to use. You can use the <a href="/docs/api-reference/models/list">List models</a> API to see all of your available models, or see our <a href="/docs/models/overview">Model overview</a> for descriptions of them.\n&quot;,
&quot;x-oaiTypeLabel&quot;: &quot;string&quot;
},
&quot;prompt&quot;: {
&quot;oneOf&quot;: [
{
&quot;type&quot;: &quot;string&quot;,
&quot;default&quot;: &quot;&quot;,
&quot;example&quot;: &quot;This is a test.&quot;
},
{
&quot;type&quot;: &quot;array&quot;,
&quot;items&quot;: {
&quot;type&quot;: &quot;string&quot;,
&quot;default&quot;: &quot;&quot;,
&quot;example&quot;: &quot;This is a test.&quot;
}
},
{
&quot;minItems&quot;: 1,
&quot;type&quot;: &quot;array&quot;,
&quot;items&quot;: {
&quot;type&quot;: &quot;integer&quot;
},
&quot;example&quot;: &quot;[1212, 318, 257, 1332, 13]&quot;
},
{
&quot;minItems&quot;: 1,
&quot;type&quot;: &quot;array&quot;,
&quot;items&quot;: {
&quot;minItems&quot;: 1,
&quot;type&quot;: &quot;array&quot;,
&quot;items&quot;: {
&quot;type&quot;: &quot;integer&quot;
}
},
&quot;example&quot;: &quot;[[1212, 318, 257, 1332, 13]]&quot;
}
],
&quot;description&quot;: &quot;The prompt(s) to generate completions for, encoded as a string, array of strings, array of tokens, or array of token arrays.\n\nNote that &lt;|endoftext|&gt; is the document separator that the model sees during training, so if a prompt is not specified the model will generate as if from the beginning of a new document.\n&quot;,
&quot;default&quot;: &quot;&lt;|endoftext|&gt;&quot;,
&quot;nullable&quot;: true
},
&quot;best_of&quot;: {
&quot;maximum&quot;: 20,
&quot;minimum&quot;: 0,
&quot;type&quot;: &quot;integer&quot;,
&quot;description&quot;: &quot;Generates <code>best_of</code> completions server-side and returns the &quot;best&quot; (the one with the highest log probability per token). Results cannot be streamed.\n\nWhen used with <code>n</code>, <code>best_of</code> controls the number of candidate completions and <code>n</code> specifies how many to return – <code>best_of</code> must be greater than <code>n</code>.\n\n<strong>Note:</strong> Because this parameter generates many completions, it can quickly consume your token quota. Use carefully and ensure that you have reasonable settings for <code>max_tokens</code> and <code>stop</code>.\n&quot;,
&quot;default&quot;: 1,
&quot;nullable&quot;: true
},
&quot;echo&quot;: {
&quot;type&quot;: &quot;boolean&quot;,
&quot;description&quot;: &quot;Echo back the prompt in addition to the completion\n&quot;,
&quot;default&quot;: false,
&quot;nullable&quot;: true
},
&quot;frequency_penalty&quot;: {
&quot;maximum&quot;: 2,
&quot;minimum&quot;: -2,
&quot;type&quot;: &quot;number&quot;,
&quot;description&quot;: &quot;Number between -2.0 and 2.0. Positive values penalize new tokens based on their existing frequency in the text so far, decreasing the model's likelihood to repeat the same line verbatim.\n\n<a href="/docs/guides/text-generation/parameter-details">See more information about frequency and presence penalties.</a>\n&quot;,
&quot;default&quot;: 0,
&quot;nullable&quot;: true
},
&quot;logit_bias&quot;: {
&quot;type&quot;: &quot;object&quot;,
&quot;additionalProperties&quot;: {
&quot;type&quot;: &quot;integer&quot;
},
&quot;description&quot;: &quot;Modify the likelihood of specified tokens appearing in the completion.\n\nAccepts a JSON object that maps tokens (specified by their token ID in the GPT tokenizer) to an associated bias value from -100 to 100. You can use this <a href="/tokenizer?view=bpe">tokenizer tool</a> (which works for both GPT-2 and GPT-3) to convert text to token IDs. Mathematically, the bias is added to the logits generated by the model prior to sampling. The exact effect will vary per model, but values between -1 and 1 should decrease or increase likelihood of selection; values like -100 or 100 should result in a ban or exclusive selection of the relevant token.\n\nAs an example, you can pass <code>{\&quot;50256\&quot;: -100}</code> to prevent the &lt;|endoftext|&gt; token from being generated.\n&quot;,
&quot;default&quot;: null,
&quot;nullable&quot;: true,
&quot;x-oaiTypeLabel&quot;: &quot;map&quot;
},
&quot;logprobs&quot;: {
&quot;maximum&quot;: 5,
&quot;minimum&quot;: 0,
&quot;type&quot;: &quot;integer&quot;,
&quot;description&quot;: &quot;Include the log probabilities on the <code>logprobs</code> most likely output tokens, as well the chosen tokens. For example, if <code>logprobs</code> is 5, the API will return a list of the 5 most likely tokens. The API will always return the <code>logprob</code> of the sampled token, so there may be up to <code>logprobs+1</code> elements in the response.\n\nThe maximum value for <code>logprobs</code> is 5.\n&quot;,
&quot;default&quot;: null,
&quot;nullable&quot;: true
},
&quot;max_tokens&quot;: {
&quot;minimum&quot;: 0,
&quot;type&quot;: &quot;integer&quot;,
&quot;description&quot;: &quot;The maximum number of <a href="/tokenizer">tokens</a> that can be generated in the completion.\n\nThe token count of your prompt plus <code>max_tokens</code> cannot exceed the model's context length. <a href="https://cookbook.openai.com/examples/how_to_count_tokens_with_tiktoken">Example Python code</a> for counting tokens.\n&quot;,
&quot;default&quot;: 16,
&quot;nullable&quot;: true,
&quot;example&quot;: 16
},
&quot;n&quot;: {
&quot;maximum&quot;: 128,
&quot;minimum&quot;: 1,
&quot;type&quot;: &quot;integer&quot;,
&quot;description&quot;: &quot;How many completions to generate for each prompt.\n\n<strong>Note:</strong> Because this parameter generates many completions, it can quickly consume your token quota. Use carefully and ensure that you have reasonable settings for <code>max_tokens</code> and <code>stop</code>.\n&quot;,
&quot;default&quot;: 1,
&quot;nullable&quot;: true,
&quot;example&quot;: 1
},
&quot;presence_penalty&quot;: {
&quot;maximum&quot;: 2,
&quot;minimum&quot;: -2,
&quot;type&quot;: &quot;number&quot;,
&quot;description&quot;: &quot;Number between -2.0 and 2.0. Positive values penalize new tokens based on whether they appear in the text so far, increasing the model's likelihood to talk about new topics.\n\n<a href="/docs/guides/text-generation/parameter-details">See more information about frequency and presence penalties.</a>\n&quot;,
&quot;default&quot;: 0,
&quot;nullable&quot;: true
},
&quot;seed&quot;: {
&quot;maximum&quot;: 9223372036854775807,
&quot;minimum&quot;: -9223372036854775808,
&quot;type&quot;: &quot;integer&quot;,
&quot;description&quot;: &quot;If specified, our system will make a best effort to sample deterministically, such that repeated requests with the same <code>seed</code> and parameters should return the same result.\n\nDeterminism is not guaranteed, and you should refer to the <code>system_fingerprint</code> response parameter to monitor changes in the backend.\n&quot;,
&quot;nullable&quot;: true
},
&quot;stop&quot;: {
&quot;oneOf&quot;: [
{
&quot;type&quot;: &quot;string&quot;,
&quot;default&quot;: &quot;&lt;|endoftext|&gt;&quot;,
&quot;nullable&quot;: true,
&quot;example&quot;: &quot;\n&quot;
},
{
&quot;maxItems&quot;: 4,
&quot;minItems&quot;: 1,
&quot;type&quot;: &quot;array&quot;,
&quot;items&quot;: {
&quot;type&quot;: &quot;string&quot;,
&quot;example&quot;: &quot;[&quot;\n&quot;]&quot;
}
}
],
&quot;description&quot;: &quot;Up to 4 sequences where the API will stop generating further tokens. The returned text will not contain the stop sequence.\n&quot;,
&quot;default&quot;: null,
&quot;nullable&quot;: true
},
&quot;stream&quot;: {
&quot;type&quot;: &quot;boolean&quot;,
&quot;description&quot;: &quot;Whether to stream back partial progress. If set, tokens will be sent as data-only <a href="https://developer.mozilla.org/en-US/docs/Web/API/Server-sent_events/Using_server-sent_events#Event_stream_format">server-sent events</a> as they become available, with the stream terminated by a <code>data: [DONE]</code> message. <a href="https://cookbook.openai.com/examples/how_to_stream_completions">Example Python code</a>.\n&quot;,
&quot;default&quot;: false,
&quot;nullable&quot;: true
},
&quot;suffix&quot;: {
&quot;type&quot;: &quot;string&quot;,
&quot;description&quot;: &quot;The suffix that comes after a completion of inserted text.&quot;,
&quot;default&quot;: null,
&quot;nullable&quot;: true,
&quot;example&quot;: &quot;test.&quot;
},
&quot;temperature&quot;: {
&quot;maximum&quot;: 2,
&quot;minimum&quot;: 0,
&quot;type&quot;: &quot;number&quot;,
&quot;description&quot;: &quot;What sampling temperature to use, between 0 and 2. Higher values like 0.8 will make the output more random, while lower values like 0.2 will make it more focused and deterministic.\n\nWe generally recommend altering this or <code>top_p</code> but not both.\n&quot;,
&quot;default&quot;: 1,
&quot;nullable&quot;: true,
&quot;example&quot;: 1
},
&quot;top_p&quot;: {
&quot;maximum&quot;: 1,
&quot;minimum&quot;: 0,
&quot;type&quot;: &quot;number&quot;,
&quot;description&quot;: &quot;An alternative to sampling with temperature, called nucleus sampling, where the model considers the results of the tokens with top_p probability mass. So 0.1 means only the tokens comprising the top 10% probability mass are considered.\n\nWe generally recommend altering this or <code>temperature</code> but not both.\n&quot;,
&quot;default&quot;: 1,
&quot;nullable&quot;: true,
&quot;example&quot;: 1
},
&quot;user&quot;: {
&quot;type&quot;: &quot;string&quot;,
&quot;description&quot;: &quot;A unique identifier representing your end-user, which can help OpenAI to monitor and detect abuse. <a href="/docs/guides/safety-best-practices/end-user-ids">Learn more</a>.\n&quot;,
&quot;example&quot;: &quot;user-1234&quot;
}
}
}</pre></section><section class="responses"><h2>Responses</h2><ul class="responses"><li class="response"><span class="statusLine">200</span> <span class="statusDescription">OK</span></li></ul></section></article></body></html>

</article>

        <div class="contribution d-print-none">
          <a href="https://github.com/darrelmiller/httpdoc/blob/main/examples/openai/createCompletion.md/#L1" class="edit-link">Edit this page</a>
        </div>

        <div class="next-article d-print-none border-top" id="nextArticle"></div>

      </div>

      <div class="affix">
        <nav id="affix"></nav>
      </div>
    </main>


    <footer class="border-top text-secondary">
      <div class="container-xxl">
        <div class="flex-fill">
          <span>Made with <a href="https://dotnet.github.io/docfx">docfx</a></span>
        </div>
      </div>
    </footer>
  </body>
</html>
