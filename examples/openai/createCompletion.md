<!DOCTYPE html><html><head><title>Creates a completion for the provided prompt and parameters.</title><link rel="stylesheet" href="./OpenApi.css"/><meta charset="utf-8"/><meta name="viewport" content="width=device-width, initial-scale=1"/></head><body><article><section class="requestOverview"><h1 class="request-summary">Creates a completion for the provided prompt and parameters.</h1></section><section class="http"><h3>HTTP Request</h3><pre class="http-example"><span class="request-line">POST</span> <span class="http-target">\completions</span> <span class="http-version">HTTP/1.1</span>&#xA;<span class="header-line">host</span>: <span class="header-value">api.openai.com:443</span>&#xA;<span class="header-line">accept</span>: <span class="header-value">application/json</span>&#xA;<span class="header-line">content-type</span>: <span class="header-value">application/json</span>&#xA;&#xA;null</pre></section><section class="requestContent"><h3>Request Body Schema</h3><pre class="schema">{&#xA;  &quot;required&quot;: [&#xA;    &quot;model&quot;,&#xA;    &quot;prompt&quot;&#xA;  ],&#xA;  &quot;type&quot;: &quot;object&quot;,&#xA;  &quot;properties&quot;: {&#xA;    &quot;model&quot;: {&#xA;      &quot;anyOf&quot;: [&#xA;        {&#xA;          &quot;type&quot;: &quot;string&quot;&#xA;        },&#xA;        {&#xA;          &quot;enum&quot;: [&#xA;            &quot;babbage-002&quot;,&#xA;            &quot;davinci-002&quot;,&#xA;            &quot;gpt-3.5-turbo-instruct&quot;,&#xA;            &quot;text-davinci-003&quot;,&#xA;            &quot;text-davinci-002&quot;,&#xA;            &quot;text-davinci-001&quot;,&#xA;            &quot;code-davinci-002&quot;,&#xA;            &quot;text-curie-001&quot;,&#xA;            &quot;text-babbage-001&quot;,&#xA;            &quot;text-ada-001&quot;&#xA;          ],&#xA;          &quot;type&quot;: &quot;string&quot;&#xA;        }&#xA;      ],&#xA;      &quot;description&quot;: &quot;ID of the model to use. You can use the [List models](/docs/api-reference/models/list) API to see all of your available models, or see our [Model overview](/docs/models/overview) for descriptions of them.\n&quot;,&#xA;      &quot;x-oaiTypeLabel&quot;: &quot;string&quot;&#xA;    },&#xA;    &quot;prompt&quot;: {&#xA;      &quot;oneOf&quot;: [&#xA;        {&#xA;          &quot;type&quot;: &quot;string&quot;,&#xA;          &quot;default&quot;: &quot;&quot;,&#xA;          &quot;example&quot;: &quot;This is a test.&quot;&#xA;        },&#xA;        {&#xA;          &quot;type&quot;: &quot;array&quot;,&#xA;          &quot;items&quot;: {&#xA;            &quot;type&quot;: &quot;string&quot;,&#xA;            &quot;default&quot;: &quot;&quot;,&#xA;            &quot;example&quot;: &quot;This is a test.&quot;&#xA;          }&#xA;        },&#xA;        {&#xA;          &quot;minItems&quot;: 1,&#xA;          &quot;type&quot;: &quot;array&quot;,&#xA;          &quot;items&quot;: {&#xA;            &quot;type&quot;: &quot;integer&quot;&#xA;          },&#xA;          &quot;example&quot;: &quot;[1212, 318, 257, 1332, 13]&quot;&#xA;        },&#xA;        {&#xA;          &quot;minItems&quot;: 1,&#xA;          &quot;type&quot;: &quot;array&quot;,&#xA;          &quot;items&quot;: {&#xA;            &quot;minItems&quot;: 1,&#xA;            &quot;type&quot;: &quot;array&quot;,&#xA;            &quot;items&quot;: {&#xA;              &quot;type&quot;: &quot;integer&quot;&#xA;            }&#xA;          },&#xA;          &quot;example&quot;: &quot;[[1212, 318, 257, 1332, 13]]&quot;&#xA;        }&#xA;      ],&#xA;      &quot;description&quot;: &quot;The prompt(s) to generate completions for, encoded as a string, array of strings, array of tokens, or array of token arrays.\n\nNote that &lt;|endoftext|&gt; is the document separator that the model sees during training, so if a prompt is not specified the model will generate as if from the beginning of a new document.\n&quot;,&#xA;      &quot;default&quot;: &quot;&lt;|endoftext|&gt;&quot;,&#xA;      &quot;nullable&quot;: true&#xA;    },&#xA;    &quot;best_of&quot;: {&#xA;      &quot;maximum&quot;: 20,&#xA;      &quot;minimum&quot;: 0,&#xA;      &quot;type&quot;: &quot;integer&quot;,&#xA;      &quot;description&quot;: &quot;Generates `best_of` completions server-side and returns the \&quot;best\&quot; (the one with the highest log probability per token). Results cannot be streamed.\n\nWhen used with `n`, `best_of` controls the number of candidate completions and `n` specifies how many to return &#x2013; `best_of` must be greater than `n`.\n\n**Note:** Because this parameter generates many completions, it can quickly consume your token quota. Use carefully and ensure that you have reasonable settings for `max_tokens` and `stop`.\n&quot;,&#xA;      &quot;default&quot;: 1,&#xA;      &quot;nullable&quot;: true&#xA;    },&#xA;    &quot;echo&quot;: {&#xA;      &quot;type&quot;: &quot;boolean&quot;,&#xA;      &quot;description&quot;: &quot;Echo back the prompt in addition to the completion\n&quot;,&#xA;      &quot;default&quot;: false,&#xA;      &quot;nullable&quot;: true&#xA;    },&#xA;    &quot;frequency_penalty&quot;: {&#xA;      &quot;maximum&quot;: 2,&#xA;      &quot;minimum&quot;: -2,&#xA;      &quot;type&quot;: &quot;number&quot;,&#xA;      &quot;description&quot;: &quot;Number between -2.0 and 2.0. Positive values penalize new tokens based on their existing frequency in the text so far, decreasing the model&#x27;s likelihood to repeat the same line verbatim.\n\n[See more information about frequency and presence penalties.](/docs/guides/text-generation/parameter-details)\n&quot;,&#xA;      &quot;default&quot;: 0,&#xA;      &quot;nullable&quot;: true&#xA;    },&#xA;    &quot;logit_bias&quot;: {&#xA;      &quot;type&quot;: &quot;object&quot;,&#xA;      &quot;additionalProperties&quot;: {&#xA;        &quot;type&quot;: &quot;integer&quot;&#xA;      },&#xA;      &quot;description&quot;: &quot;Modify the likelihood of specified tokens appearing in the completion.\n\nAccepts a JSON object that maps tokens (specified by their token ID in the GPT tokenizer) to an associated bias value from -100 to 100. You can use this [tokenizer tool](/tokenizer?view=bpe) (which works for both GPT-2 and GPT-3) to convert text to token IDs. Mathematically, the bias is added to the logits generated by the model prior to sampling. The exact effect will vary per model, but values between -1 and 1 should decrease or increase likelihood of selection; values like -100 or 100 should result in a ban or exclusive selection of the relevant token.\n\nAs an example, you can pass `{\&quot;50256\&quot;: -100}` to prevent the &lt;|endoftext|&gt; token from being generated.\n&quot;,&#xA;      &quot;default&quot;: null,&#xA;      &quot;nullable&quot;: true,&#xA;      &quot;x-oaiTypeLabel&quot;: &quot;map&quot;&#xA;    },&#xA;    &quot;logprobs&quot;: {&#xA;      &quot;maximum&quot;: 5,&#xA;      &quot;minimum&quot;: 0,&#xA;      &quot;type&quot;: &quot;integer&quot;,&#xA;      &quot;description&quot;: &quot;Include the log probabilities on the `logprobs` most likely output tokens, as well the chosen tokens. For example, if `logprobs` is 5, the API will return a list of the 5 most likely tokens. The API will always return the `logprob` of the sampled token, so there may be up to `logprobs&#x2B;1` elements in the response.\n\nThe maximum value for `logprobs` is 5.\n&quot;,&#xA;      &quot;default&quot;: null,&#xA;      &quot;nullable&quot;: true&#xA;    },&#xA;    &quot;max_tokens&quot;: {&#xA;      &quot;minimum&quot;: 0,&#xA;      &quot;type&quot;: &quot;integer&quot;,&#xA;      &quot;description&quot;: &quot;The maximum number of [tokens](/tokenizer) that can be generated in the completion.\n\nThe token count of your prompt plus `max_tokens` cannot exceed the model&#x27;s context length. [Example Python code](https://cookbook.openai.com/examples/how_to_count_tokens_with_tiktoken) for counting tokens.\n&quot;,&#xA;      &quot;default&quot;: 16,&#xA;      &quot;nullable&quot;: true,&#xA;      &quot;example&quot;: 16&#xA;    },&#xA;    &quot;n&quot;: {&#xA;      &quot;maximum&quot;: 128,&#xA;      &quot;minimum&quot;: 1,&#xA;      &quot;type&quot;: &quot;integer&quot;,&#xA;      &quot;description&quot;: &quot;How many completions to generate for each prompt.\n\n**Note:** Because this parameter generates many completions, it can quickly consume your token quota. Use carefully and ensure that you have reasonable settings for `max_tokens` and `stop`.\n&quot;,&#xA;      &quot;default&quot;: 1,&#xA;      &quot;nullable&quot;: true,&#xA;      &quot;example&quot;: 1&#xA;    },&#xA;    &quot;presence_penalty&quot;: {&#xA;      &quot;maximum&quot;: 2,&#xA;      &quot;minimum&quot;: -2,&#xA;      &quot;type&quot;: &quot;number&quot;,&#xA;      &quot;description&quot;: &quot;Number between -2.0 and 2.0. Positive values penalize new tokens based on whether they appear in the text so far, increasing the model&#x27;s likelihood to talk about new topics.\n\n[See more information about frequency and presence penalties.](/docs/guides/text-generation/parameter-details)\n&quot;,&#xA;      &quot;default&quot;: 0,&#xA;      &quot;nullable&quot;: true&#xA;    },&#xA;    &quot;seed&quot;: {&#xA;      &quot;maximum&quot;: 9223372036854775807,&#xA;      &quot;minimum&quot;: -9223372036854775808,&#xA;      &quot;type&quot;: &quot;integer&quot;,&#xA;      &quot;description&quot;: &quot;If specified, our system will make a best effort to sample deterministically, such that repeated requests with the same `seed` and parameters should return the same result.\n\nDeterminism is not guaranteed, and you should refer to the `system_fingerprint` response parameter to monitor changes in the backend.\n&quot;,&#xA;      &quot;nullable&quot;: true&#xA;    },&#xA;    &quot;stop&quot;: {&#xA;      &quot;oneOf&quot;: [&#xA;        {&#xA;          &quot;type&quot;: &quot;string&quot;,&#xA;          &quot;default&quot;: &quot;&lt;|endoftext|&gt;&quot;,&#xA;          &quot;nullable&quot;: true,&#xA;          &quot;example&quot;: &quot;\n&quot;&#xA;        },&#xA;        {&#xA;          &quot;maxItems&quot;: 4,&#xA;          &quot;minItems&quot;: 1,&#xA;          &quot;type&quot;: &quot;array&quot;,&#xA;          &quot;items&quot;: {&#xA;            &quot;type&quot;: &quot;string&quot;,&#xA;            &quot;example&quot;: &quot;[\&quot;\\n\&quot;]&quot;&#xA;          }&#xA;        }&#xA;      ],&#xA;      &quot;description&quot;: &quot;Up to 4 sequences where the API will stop generating further tokens. The returned text will not contain the stop sequence.\n&quot;,&#xA;      &quot;default&quot;: null,&#xA;      &quot;nullable&quot;: true&#xA;    },&#xA;    &quot;stream&quot;: {&#xA;      &quot;type&quot;: &quot;boolean&quot;,&#xA;      &quot;description&quot;: &quot;Whether to stream back partial progress. If set, tokens will be sent as data-only [server-sent events](https://developer.mozilla.org/en-US/docs/Web/API/Server-sent_events/Using_server-sent_events#Event_stream_format) as they become available, with the stream terminated by a `data: [DONE]` message. [Example Python code](https://cookbook.openai.com/examples/how_to_stream_completions).\n&quot;,&#xA;      &quot;default&quot;: false,&#xA;      &quot;nullable&quot;: true&#xA;    },&#xA;    &quot;suffix&quot;: {&#xA;      &quot;type&quot;: &quot;string&quot;,&#xA;      &quot;description&quot;: &quot;The suffix that comes after a completion of inserted text.&quot;,&#xA;      &quot;default&quot;: null,&#xA;      &quot;nullable&quot;: true,&#xA;      &quot;example&quot;: &quot;test.&quot;&#xA;    },&#xA;    &quot;temperature&quot;: {&#xA;      &quot;maximum&quot;: 2,&#xA;      &quot;minimum&quot;: 0,&#xA;      &quot;type&quot;: &quot;number&quot;,&#xA;      &quot;description&quot;: &quot;What sampling temperature to use, between 0 and 2. Higher values like 0.8 will make the output more random, while lower values like 0.2 will make it more focused and deterministic.\n\nWe generally recommend altering this or `top_p` but not both.\n&quot;,&#xA;      &quot;default&quot;: 1,&#xA;      &quot;nullable&quot;: true,&#xA;      &quot;example&quot;: 1&#xA;    },&#xA;    &quot;top_p&quot;: {&#xA;      &quot;maximum&quot;: 1,&#xA;      &quot;minimum&quot;: 0,&#xA;      &quot;type&quot;: &quot;number&quot;,&#xA;      &quot;description&quot;: &quot;An alternative to sampling with temperature, called nucleus sampling, where the model considers the results of the tokens with top_p probability mass. So 0.1 means only the tokens comprising the top 10% probability mass are considered.\n\nWe generally recommend altering this or `temperature` but not both.\n&quot;,&#xA;      &quot;default&quot;: 1,&#xA;      &quot;nullable&quot;: true,&#xA;      &quot;example&quot;: 1&#xA;    },&#xA;    &quot;user&quot;: {&#xA;      &quot;type&quot;: &quot;string&quot;,&#xA;      &quot;description&quot;: &quot;A unique identifier representing your end-user, which can help OpenAI to monitor and detect abuse. [Learn more](/docs/guides/safety-best-practices/end-user-ids).\n&quot;,&#xA;      &quot;example&quot;: &quot;user-1234&quot;&#xA;    }&#xA;  }&#xA;}</pre></section><section class="responses"><h2>Responses</h2><ul class="responses"><li class="response"><pre class="http-example"><span class="status-line">200</span> <span class="status-description">OK</span>
<span class="header-line">content-type</span>: <span class="header-value">application/json</span>&#xA;&#xA;null</pre></li></ul></section></article></body></html>